<footer class="footer mt-5 bg-light text-dark">
    <div class="container-fluid">
        <div class="row">
            <div class="col-md-4 mb-3">
                <h5>About Us</h5>
                <p>We are dedicated to providing tools for benchmarking and optimizing local LLM performance. Our goal is to help developers make informed decisions about their hardware setups.</p>
            </div>
            <div class="col-md-4 mb-3">
                <h5>Quick Links</h5>
                <ul class="list-unstyled">
                    <li><a href="#about" class="text-dark">About the Simulator</a></li>
                    <li><a href="#usage" class="text-dark">How to Use</a></li>
                    <li><a href="#contact" class="text-dark">Contact Us</a></li>
                    <li><a href="https://github.com/kamilstanuch" target="_blank" class="text-dark">GitHub Profile</a></li>
                </ul>
            </div>
            <div class="col-md-4 mb-3">
                <h5>Resources</h5>
                <ul class="list-unstyled">
                    <li><a href="https://github.com/ggerganov/llama.cpp/discussions/4167" target="_blank" class="text-dark">Performance Analysis</a></li>
                    <li><a href="https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference" target="_blank" class="text-dark">GPU Benchmarks</a></li>
                    <li><a href="#faq" class="text-dark">FAQ</a></li>
                    <li><a href="#terms" class="text-dark">Terms of Service</a></li>
                </ul>
            </div>
        </div>
        <div class="text-center mt-4">
            <p class="mb-0">Â© 2023 LLM Token Generation Simulator. All rights reserved.</p>
            <p class="mb-0">
                Created by <a href="https://github.com/kamilstanuch" target="_blank">Kamil Stanuch</a>
            </p>
        </div>
    </div>
</footer>